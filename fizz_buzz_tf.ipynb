{"cells":[{"cell_type":"markdown","source":["# Demonstration of TensorFlow: FizzBuzz\n\nThis notebook is a demonstration of how to solve the FuzzBuzz problem using a neural network. Big thanks to JoelGrus for the TF code. This notebook is paired with the presentation, 'Deep Learning Explained for Developers'."],"metadata":{}},{"cell_type":"markdown","source":["### Install necessary libraries"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport tensorflow as tf"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Step 1: How will we represent our data?\nTo generate some sample data, we'll convert each number into a binary vector. This will make our data much more compatible for a neural network. \n\n**Input**\nA binary representation of a number. For example: [0, 1, 0, 1] <<-- the number '5'\n  \n**Output** \nThe probabilities of: \n  a. The number itself.\n  b. 'Fizz'\n  c. 'Buzz'\n  d. 'FizzBuzz'\n  \nFor example: [0.1, 0.5, 0.1, 0.3]"],"metadata":{}},{"cell_type":"code","source":["def binary_encode(i, num_digits):\n    return np.array([i >> d & 1 for d in range(num_digits)])\n  \ndef fizz_buzz_encode(i):\n  if   i % 15 == 0: return np.array([0, 0, 0, 1])\n  elif i % 5  == 0: return np.array([0, 0, 1, 0])\n  elif i % 3  == 0: return np.array([0, 1, 0, 0])\n  else:             return np.array([1, 0, 0, 0])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Step 2: Generate Sample Data!\n\nNote: We need all the data (samples and otherwise) to be in the same format. Thus, we need to decide how many digits we'll consider.  Also, we don't want to use our sample data in our model."],"metadata":{}},{"cell_type":"code","source":["NUM_DIGITS = 10\ntrX = np.array([binary_encode(i, NUM_DIGITS) for i in range(101, 2 ** NUM_DIGITS)])\ntrY = np.array([fizz_buzz_encode(i)          for i in range(101, 2 ** NUM_DIGITS)])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Time to setup our model! \n\n1. We choose a number of hidden units and layers to use. \n2. We define the input and output formats, and make some choices on how to 'activate' each layer."],"metadata":{}},{"cell_type":"code","source":["NUM_HIDDEN = 100\n# An input variable with width NUM_DIGITS digits, and output variable with length 4\nX = tf.placeholder(\"float\", [None, NUM_DIGITS])\nY = tf.placeholder(\"float\", [None, 4])"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["We'll need an input variable with width NUM_DIGITS, and an output variable with width 4:"],"metadata":{}},{"cell_type":"code","source":["# X = tf.placeholder(\"float\", [None, NUM_DIGITS])\n# Y = tf.placeholder(\"float\", [None, 4])\n\n# Initialize weights randomly around 0.\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\n# Our hidden layer, and output layer\n# Weights shape: [10, 100]\nw_h = init_weights([NUM_DIGITS, NUM_HIDDEN])\nw_o = init_weights([NUM_HIDDEN, 4])\n\n# Our model has one hidden layer, and one output layer\ndef model(X, w_h, w_o):\n    h = tf.nn.relu(tf.matmul(X, w_h))\n    return tf.matmul(h, w_o)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Let the learning begin!\n\nNow that we have our model, we can try to watch it \"learn\" from our data. In other words, \"minimize error\". For the more technially inclined, we're using softmax cross-entropy as our cost function."],"metadata":{"collapsed":true}},{"cell_type":"code","source":["py_x = model(X, w_h, w_o)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost)\n\n# Our prediction will simply be the max probability of our output vector\npredict_op = tf.argmax(py_x, 1)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Lets run \nNUM_EPOCHS = 10000\nBATCH_SIZE = 128\n\n# Lets shuffle each iteration\nsess = tf.Session()\n\nwith sess.as_default():\n    tf.global_variables_initializer().run()\n    \n    for epoch in range(NUM_EPOCHS):\n        # Shuffle each input set\n        p = np.random.permutation(range(len(trX)))\n        trX, trY = trX[p], trY[p]\n        \n        # Grab batches of size BATCH_SIZE and do some training!\n        for start in range(0, len(trX), BATCH_SIZE):\n            end = start + BATCH_SIZE\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n            \n        accuracy = np.mean(np.argmax(trY, axis=1) ==\n                           sess.run(predict_op, feed_dict={X: trX, Y: trY}))\n        print(epoch, accuracy)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Make Predictions on 1 to 100!"],"metadata":{}},{"cell_type":"code","source":["# Create a list of the numbers 1 to 100, binary encode them\nnumbers = np.arange(1, 101)\nteX = np.transpose(binary_encode(numbers, NUM_DIGITS))\n\ndef fizz_buzz(i, prediction):\n    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]\n  \nteY = sess.run(predict_op, feed_dict={X: teX})\noutput = np.vectorize(fizz_buzz)(numbers, teY)\n\nfor i in range(1, len(output)):\n  print(str(i) + \":  \" + str(output[i-1]))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"},"name":"fizz_buzz_tf","notebookId":4102809021269032},"nbformat":4,"nbformat_minor":0}
